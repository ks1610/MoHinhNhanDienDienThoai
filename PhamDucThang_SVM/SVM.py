import os
import cv2
import numpy as np
import random
import matplotlib.pyplot as plt
from sklearn.svm import SVC
# <<< THÃŠM Má»šI: Import confusion_matrix vÃ  ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
# >>> Káº¾T THÃšC THÃŠM Má»šI
from sklearn.model_selection import GridSearchCV
from sklearn.decomposition import PCA
import torch
from skimage.feature import hog
# from skimage import exposure # Bá» comment nÃ y náº¿u báº¡n muá»‘n xem áº£nh HOG

# =========================
# 1. HÃ€M TRÃCH XUáº¤T Äáº¶C TRÆ¯NG HOG (THAY THáº¾)
# =========================
def extract_features(img_path):
    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        print(f"âš ï¸ KhÃ´ng thá»ƒ Ä‘á»c áº£nh: {img_path}")
        return None

 
    img = cv2.resize(img, (128, 128))

   
    features, hog_image = hog(img, 
                              orientations=9, 
                              pixels_per_cell=(16, 16),
                              cells_per_block=(2, 2), 
                              visualize=True, 
                              block_norm='L2-Hys')

   

   
    std_dev = np.std(img)
    
    
    combined_features = np.hstack((features, std_dev))
    
    return combined_features


# =========================
# 2. HÃ€M Táº¢I Dá»® LIá»†U Tá»ª CÃC THÆ¯ Má»¤C ÄÃƒ CHIA (ÄÃƒ Sá»¬A Lá»–I Äá»ŒC THÆ¯ Má»¤C CON)
# =========================
def load_data_from_split(dataset_path, split_name, label_map):
    X, y, image_paths = [], [], []
    split_dir = os.path.join(dataset_path, split_name) # vd: /kaggle/input/dataset/train

    if not os.path.exists(split_dir):
        print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c: {split_dir}")
        return np.array(X), np.array(y), image_paths

    # Láº·p qua cÃ¡c thÆ° má»¥c lá»›p chÃ­nh ('non-phone', 'defective', ...)
    for label_name, label_id in label_map.items():
        class_dir = os.path.join(split_dir, label_name)
        if not os.path.exists(class_dir):
            print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c con: {class_dir}")
            continue

        # Láº·p thÃªm má»™t cáº¥p thÆ° má»¥c con bÃªn trong (vd: 'hop', 'tivi', 'screen-on')
        for sub_dir_name in os.listdir(class_dir):
            sub_dir_path = os.path.join(class_dir, sub_dir_name)
            
            # Äáº£m báº£o ráº±ng Ä‘Ã³ lÃ  má»™t thÆ° má»¥c
            if os.path.isdir(sub_dir_path):
                # BÃ¢y giá» má»›i láº·p qua cÃ¡c file áº£nh bÃªn trong thÆ° má»¥c con nÃ y
                for filename in os.listdir(sub_dir_path):
                    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
                        file_path = os.path.join(sub_dir_path, filename)
                        features = extract_features(file_path)
                        
                        if features is not None:
                            X.append(features)
                            y.append(label_id)
                            image_paths.append(file_path)
            else:
                # Náº¿u cÃ³ file áº£nh náº±m ngay ngoÃ i, cÅ©ng Ä‘á»c luÃ´n
                if sub_dir_name.lower().endswith(('.png', '.jpg', '.jpeg')):
                    file_path = os.path.join(class_dir, sub_dir_name)
                    features = extract_features(file_path)
                    if features is not None:
                        X.append(features)
                        y.append(label_id)
                        image_paths.append(file_path)

    return np.array(X), np.array(y), image_paths



# =========================
# 3. Váº¼ PHÃ‚N Bá» Dá»® LIá»†U 
# =========================
def plot_dataset_distribution(train_size, val_size, test_size):
    labels = ['Train', 'Validation', 'Test']
    sizes = [train_size, val_size, test_size]

    plt.figure(figsize=(6, 4))
    bars = plt.bar(labels, sizes, color=['#4CAF50', '#FFC107', '#2196F3'])
    plt.title('PhÃ¢n bá»‘ dá»¯ liá»‡u Train / Validation / Test')
    plt.ylabel('Sá»‘ lÆ°á»£ng máº«u')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    for bar in bars:
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, yval + 2, int(yval), ha='center', va='bottom')
    plt.show()


# =========================
# 4. Dá»° ÄOÃN áº¢NH Má»šI 
# =========================
def predict_image(model, img_path, class_names):
    features = extract_features(img_path)
    if features is None:
        return
    
    if features.ndim == 1:
        features = features.reshape(1, -1)
        
    result_id = model.predict(features)[0]
    result_name = class_names[result_id]
    
    if result_id == 2: # 'defective'
        print(f"ğŸ“¸ {os.path.basename(img_path)} â†’ âš ï¸ Bá»Š Vá»  ({result_name})")
    elif result_id == 1: # 'non defective'
        print(f"ğŸ“¸ {os.path.basename(img_path)} â†’ âœ… BÃŒNH THÆ¯á»œNG ({result_name})")
    else: # 'non-phone'
        print(f"ğŸ“¸ {os.path.basename(img_path)} â†’ âŒ KHÃ”NG PHáº¢I ÄIá»†N THOáº I ({result_name})")


# =========================
# 5. HÃ€M Váº¼ BIá»‚U Äá»’ Báº°NG PCA
# =========================
def plot_svm_pca_boundary(X_train, y_train, X_test, y_test, class_names):
    print("\nğŸ¨ Äang giáº£m chiá»u dá»¯ liá»‡u báº±ng PCA Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“...")
    
  
    X_full = np.vstack((X_train, X_test))
    y_full = np.hstack((y_train, y_test))
    
 
    pca = PCA(n_components=2)
    X_pca_full = pca.fit_transform(X_full)
    

    svm_2d = SVC(kernel='rbf', C=10).fit(X_pca_full, y_full)
    
    x_min, x_max = X_pca_full[:, 0].min() - 1, X_pca_full[:, 0].max() + 1
    y_min, y_max = X_pca_full[:, 1].min() - 1, X_pca_full[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                         np.arange(y_min, y_max, 0.02))
    
    Z = svm_2d.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    
    plt.figure(figsize=(10, 8))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')
    
    
    scatter = plt.scatter(X_pca_full[:, 0], X_pca_full[:, 1], c=y_full, cmap='viridis', edgecolors='k', alpha=0.7)
    handles, _ = scatter.legend_elements()
    plt.legend(handles, class_names, title="Classes")
    
    plt.title('Biá»ƒu Ä‘á»“ PCA 2-chiá»u cá»§a Ä‘áº·c trÆ°ng HOG vÃ  ranh giá»›i SVM (minh há»a)')
    plt.xlabel('ThÃ nh pháº§n chÃ­nh 1 (PC1)')
    plt.ylabel('ThÃ nh pháº§n chÃ­nh 2 (PC2)')
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.show()

# =========================
# 6.  HÃ€M Váº¼ MA TRáº¬N NHáº¦M LáºªN
# =========================
def plot_confusion_matrix(y_true, y_pred, class_names, title):
    """
    HÃ m nÃ y váº½ ma tráº­n nháº§m láº«n.
    """
  
    cm = confusion_matrix(y_true, y_pred)
    
   
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
    
   
    disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical')
    plt.title(title)
    plt.grid(False) # Táº¯t lÆ°á»›i cá»§a matplotlib Ä‘á»ƒ hiá»ƒn thá»‹ rÃµ hÆ¡n
    plt.show()

# =========================
# 7. CHáº Y TOÃ€N Bá»˜
# =========================
model_file = "/kaggle/working/svm_model.pth"
dataset_base_path = '/kaggle/input/dataset'

# Äá»‹nh nghÄ©a cÃ¡c lá»›p (khá»›p vá»›i tÃªn thÆ° má»¥c cá»§a báº¡n)
label_map = {
    'non-phone': 0,
    'non-defective': 1,
    'defective': 2
}
class_names = list(label_map.keys())

print("ğŸ”¸ Äang táº£i dá»¯ liá»‡u train...")
X_train, y_train, paths_train = load_data_from_split(dataset_base_path, 'train', label_map)
print(f"âœ… ÄÃ£ táº£i {len(X_train)} áº£nh train.")

print("ğŸ”¸ Äang táº£i dá»¯ liá»‡u val...")
X_val, y_val, paths_val = load_data_from_split(dataset_base_path, 'val', label_map)
print(f"âœ… ÄÃ£ táº£i {len(X_val)} áº£nh val.")

print("ğŸ”¸ Äang táº£i dá»¯ liá»‡u test...")
X_test, y_test, paths_test = load_data_from_split(dataset_base_path, 'test', label_map)
print(f"âœ… ÄÃ£ táº£i {len(X_test)} áº£nh test.")

if len(X_train) == 0:
    raise ValueError("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u train Ä‘á»ƒ huáº¥n luyá»‡n. Vui lÃ²ng kiá»ƒm tra láº¡i Ä‘Æ°á»ng dáº«n.")

print(f"\nğŸ“Š KÃ­ch thÆ°á»›c táº­p:")
print(f"  - Train: {len(X_train)}")
print(f"  - Validation: {len(X_val)}")
print(f"  - Test: {len(X_test)}")

plot_dataset_distribution(len(X_train), len(X_val), len(X_test))

# =========================
# 8. HUáº¤N LUYá»†N Báº°NG GRIDSEARCHCV
# =========================
model = None
if os.path.exists(model_file):
    print(f"\nğŸ’¾ Äang táº£i mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n tá»« {model_file} ...")
    model = torch.load(model_file)
else:
    print("\nğŸš€ Báº¯t Ä‘áº§u huáº¥n luyá»‡n mÃ´ hÃ¬nh vÃ  tÃ¬m tham sá»‘ tá»‘t nháº¥t (GridSearchCV)...")
    
    # Äá»‹nh nghÄ©a khÃ´ng gian tham sá»‘ Ä‘á»ƒ tÃ¬m
    # ChÃºng ta tÃ¬m 'C' (Ä‘á»™ pháº¡t) vÃ  'gamma' (áº£nh hÆ°á»Ÿng cá»§a 1 Ä‘iá»ƒm)
    param_grid = {
        'C': [1, 10, 100],            # Thá»­ cÃ¡c giÃ¡ trá»‹ C
        'gamma': ['scale', 0.1, 0.01], # Thá»­ cÃ¡c giÃ¡ trá»‹ gamma
        'kernel': ['rbf']
    }
    
    # cv=3: Chia táº­p train thÃ nh 3 pháº§n Ä‘á»ƒ kiá»ƒm tra chÃ©o, tÄƒng Ä‘á»™ tin cáº­y
    # n_jobs=-1: DÃ¹ng táº¥t cáº£ CPU Ä‘á»ƒ cháº¡y song song, nhanh hÆ¡n
    grid_search = GridSearchCV(SVC(decision_function_shape='ovr'), 
                               param_grid, 
                               cv=3, 
                               verbose=2, 
                               n_jobs=-1)
    
    # Huáº¥n luyá»‡n trÃªn Táº¬P TRAIN + VALIDATION (gá»™p láº¡i Ä‘á»ƒ cÃ³ nhiá»u dá»¯ liá»‡u hÆ¡n)
    X_train_full = np.vstack((X_train, X_val))
    y_train_full = np.hstack((y_train, y_val))
    
    print(f"Huáº¥n luyá»‡n trÃªn {len(X_train_full)} máº«u (Train + Val)...")
    grid_search.fit(X_train_full, y_train_full)
    
    print("\nğŸ‰ ÄÃƒ HUáº¤N LUYá»†N XONG!")
    print(f"Tham sá»‘ tá»‘t nháº¥t tÃ¬m Ä‘Æ°á»£c: {grid_search.best_params_}")
    
    # Láº¥y ra mÃ´ hÃ¬nh tá»‘t nháº¥t
    model = grid_search.best_estimator_
    
    # ÄÃ¡nh giÃ¡ láº¡i trÃªn táº­p Validation (chá»‰ Ä‘á»ƒ tham kháº£o)
    y_val_pred = model.predict(X_val)
    print("\n=== ğŸ“Œ Káº¾T QUáº¢ VALIDATION (vá»›i mÃ´ hÃ¬nh tá»‘t nháº¥t) ===")
    print(classification_report(y_val, y_val_pred, target_names=class_names))

    # <<< THÃŠM Má»šI: Váº½ ma tráº­n nháº§m láº«n cho táº­p Validation
    plot_confusion_matrix(y_val, y_val_pred, class_names, "Ma tráº­n nháº§m láº«n - Táº¬P VALIDATION")
    # >>> Káº¾T THÃšC THÃŠM Má»šI

    # LÆ°u mÃ´ hÃ¬nh tá»‘t nháº¥t
    torch.save(model, model_file)
    print(f"ğŸ’¾ ÄÃ£ lÆ°u mÃ´ hÃ¬nh tá»‘t nháº¥t vÃ o: {model_file} (dáº¡ng .pth)")

# =========================
# 9. ÄÃNH GIÃ TRÃŠN TEST SET
# =========================
if model:
    print("\n=== ğŸ§ª Káº¾T QUáº¢ TRÃŠN TEST SET ===")
    y_test_pred = model.predict(X_test)
    print("Äá»™ chÃ­nh xÃ¡c:", accuracy_score(y_test, y_test_pred))
    print(classification_report(y_test, y_test_pred, target_names=class_names))
    
    # <<< THÃŠM Má»šI: Váº½ ma tráº­n nháº§m láº«n cho táº­p Test
    plot_confusion_matrix(y_test, y_test_pred, class_names, "Ma tráº­n nháº§m láº«n - Táº¬P TEST")
    # >>> Káº¾T THÃšC THÃŠM Má»šI

    # Váº½ biá»ƒu Ä‘á»“ PCA
    plot_svm_pca_boundary(X_train, y_train, X_test, y_test, class_names)
else:
    print("âŒ KhÃ´ng cÃ³ mÃ´ hÃ¬nh Ä‘á»ƒ Ä‘Ã¡nh giÃ¡.")